# Specification
Program written in Python3+

Used modules: scipy, numpy, numdifftools, time, random

# Assignment description 
 
Write a program that minimizes a function ğ½: ğ‘…! â†’ ğ‘… using two gradient-based methods. The optimized function should have the following form: 	ğ½(ğ‘¥) = ğ‘ + ğ‘"ğ‘¥ + ğ‘¥"ğ´ğ‘¥ where ğ‘ is a scalar number, ğ‘ is a d-dimensional real vector, and A is a positiveâ€“definite matrix â€“ all parameters are specified by the user. 
 
Your program should minimize the J function using the simple gradient descent method and Newtonâ€™s method (user gets to choose which method to use). As a result, it should return the found solution ğ‘¥âˆ— along with the function value	ğ½(ğ‘¥âˆ—). 
 
It should be possible to define the starting point for the optimization procedure in two ways: either the user can directly set the ğ‘¥$ âˆˆ ğ‘…! initial vector, or its elements get generated by drawing numbers from a uniform distribution defined for the range [ğ‘™, ğ‘¢]. 
 
The stopping condition for the optimization procedure should include the following factors: maximum number of iterations, desired J(x) value to reach (so you stop the iterative process as soon as ğ½  ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’_ğ‘¡ğ‘œ_ğ‘Ÿğ‘’ğ‘ğ‘â„ ) and maximum computation time. 
 
You should also provide a batch/restart mode â€“ a way to run your program so that the optimization gets restarted n-times and the found solutionsâ€™ mean and standard deviation are reported. This mode should work exactly the same as manually (and independently) running the optimization process for the same J(x) function multiple times. So, if the starting point is set for random generation, it changes for every run. 
